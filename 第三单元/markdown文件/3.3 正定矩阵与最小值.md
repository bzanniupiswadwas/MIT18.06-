本讲将围绕着正定矩阵这一主题串联起本门课程许多重要的内容！

# 正定矩阵
首先我们从一个最简单的$2\times 2$的矩阵开始。
如果我们有矩阵$A=\begin{bmatrix}a&b\\b&c\end{bmatrix}$。那么我们怎么去判断这个矩阵是一个正定矩阵呢？
（1）特征值都是正数。数学表达就是$\lambda_{1}>0，\lambda_{2}>0$。
（2）行列式，子行列式都大于0。数学表达为：$a>0，ac-b^2>0$。
（3）主元都大于0。数学表达为：$a>0,\frac{ac-b^2}{a}>0$。（行列式等于主元的积！）
（4）判据式（二次型）：$x^TAx>0$（这个代表着恒大于）
# 例子
关于第4点判据式，我们将有许多内容值得讨论！接下来我们举些例子说说看（包含正定矩阵，半正定矩阵，非正定矩阵）：
首先如果矩阵A是$\begin{bmatrix}2&6\\6&y\end{bmatrix}$
用行列式检验可知：当 2y − 36 > 0，即 y > 18 时，矩阵正定。
若 y = 18，则矩阵是$\begin{bmatrix}2&6\\6&18\end{bmatrix}$，位于正定与非正定的“边界”，称为**半正定矩阵**（positive semidefinite）。它是奇异矩阵，有特征值 0 和 20。半正定矩阵的所有特征值 ≥ 0。由于是奇异矩阵，其行列式为 0，且只有一个主元。然而上面我们从普通的主元，特征值，行列式角度分析了矩阵 A，那么接下来我们就运用最重要的判据式来展开讨论：

在判据式中，其中x是一个任意的向量！我们随意写出一个A矩阵出来：$\begin{bmatrix}2&6\\6&18\end{bmatrix}$
那么$x^TAx=\begin{bmatrix}x_{1}&x_{2}\end{bmatrix}\begin{bmatrix}2&6\\6&18\end{bmatrix}\begin{bmatrix}x_{1}\\x_{2}\end{bmatrix}=2x_{1}^2+12x_{1}x_{2}+18x_{2}^2$
然后其中的系数2，12，18分别是a,2b,c。在同济教材中这就是矩阵二次型。为了让大家直观些看，我们写出矩阵A$\begin{bmatrix}a&b\\b&c\end{bmatrix}$,那么显然在这个例子中a=2,c=18,b=6！这就是他们的规律所在，同样推广到更高维的矩阵也是如此，想要深入了解大家自行探索！

而所谓判定 A 是不是正定矩阵，也就是判别式$𝑥^ 𝑇Ax$ 所构造出的类似于$2x_{1}^2+12x_{1}x_{2}+18x_{2}^2$的二次型是不是恒大于 0。对于本例中的 $2x_{1}^2+12x_{1}x_{2}+18x_{2}^2$来说，它显然不是恒正的，因为在某种取值中，其结果可以为0。而半正定矩阵，它是奇异矩阵且半正定矩阵的所有特征值 ≥ 0；奇异矩阵行列式为 0，只有一个主元。在这个半正定的例子中，当 $x_{1}=3,x_{2}=−1$ 时，判据式为0！

这表明了A 为半正定时，其对应二次型会在某种情况下得到 0，如果我们把y改为7呢，那么显然我们得到的公式是：$2x_{1}^2+12x_{1}x_{2}+7x_{2}^2$。他是非正定矩阵，他的图像就有点难以想象了，是一个马鞍型，如图：
![[../../image/Pasted image 20250819232155.png]]
这还看不出来的话我们看一个网图：
![[../../image/屏幕截图 2025-08-20 122302.png]]
关于图像，我想提醒大家的是，本身来说二维矩阵的二次型一般图像是在二维空间中的，对应n维矩阵二次型图像也是在n维空间中！但是这个三维的马鞍图怎么来的，这时由于我们去了高度函数，也就是构造了$z=2x_{1}^2+12x_{1}x_{2}+7x_{2}^2$！所以图像从二维升到三维了！而他在二维空间中的图像则是一个双曲线！这个也很简单，取一个等高线，就是$k=2x_{1}^2+12x_{1}x_{2}+7x_{2}^2$。k是常数！

而当我们继续调整y值，此时的矩阵为：$\begin{bmatrix}2&6\\6&20\end{bmatrix}$,那么根据前3条定律的判定，这显然是一个正定矩阵！当我们取高度函数，那么形象的说他的三维图像就是一个碗！而三维图像的切面图是一个二次函数图像！把判定式打展开$2x_{1}^2+12x_{1}x_{2}+20x_{2}^2$，取得等高线后，在二维空间中这就是一个典型的椭圆方程式（高中死去的圆锥曲线记忆活过来的！）

而最开始我们说到的半正定矩阵的二次型，无论在二维还是在三维中，都是无研究价值的图像，我们不讨论！

那通过上面的三个例子，我们来说说极小值的判定！
# 极小值的判定
上面我们通过二次型的函数，我们分析了图像的走势。但是在解析几何中，用导数取取判断极小值点才是最好的方法！对于函数$f(x,y)=2x_{1}^2+12x_{1}x_{2}+20x_{2}^2$根据微积分中学习的知识，原点一阶偏导数为 0，二阶偏导数大于 0。故可以判断其为最小值点。就是说在微积分中，我们判断是否有最小值可以通过求导判断是否大于0，而在线性代数中，我们判断是否有最小值是通过判断二阶导数矩阵（我们后面讲）是否为正定，这是正定矩阵的用法之一，将数字转化为矩阵。

当然我们判断图像走势还可以配方。$f(x,y)=2x_{1}^2+12x_{1}x_{2}+20x_{2}7^2=2(x+3y)^2+2y^2$很容易想到，如果 A 不是正定的，那么$𝑦 ^2$前的系数势必为负数。配方法反映在线代中就是消元(也叫是LU分解)，$A=\begin{bmatrix}2&6\\6&20\end{bmatrix}$,这上的每一个元素都表示着 f(x,y)中对应项前的系数，经过消元，得到消元后的矩阵为：$A=\begin{bmatrix}2&6\\0&2\end{bmatrix}$,对应看来：
![[../../image/屏幕截图 2025-08-20 184252.png]]
  
关系显而易见。括号外的系数是主元，这样一切就联系起来了（就是第二和第三个箭头指向的系数），正定→主元为正→二次型平方项外系数为正→图像朝上→原点为最小值点。此理论可推广到 n 维。

# 二阶导数矩阵
什么是二阶导数矩阵，如下：
对于多元函数  $f(x_1,x_2,\dots,x_n)$
把所有二阶偏导数按顺序排成一个$n\times n$的矩阵，称为 **Hessian 矩阵**（二阶导数矩阵）：

$$\mathbf{H}(f)=
\begin{bmatrix}
\dfrac{\partial^{2}f}{\partial x_{1}^{2}} &
\dfrac{\partial^{2}f}{\partial x_{1}\partial x_{2}} &
\cdots &
\dfrac{\partial^{2}f}{\partial x_{1}\partial x_{n}}\\[6pt]
\dfrac{\partial^{2}f}{\partial x_{2}\partial x_{1}} &
\dfrac{\partial^{2}f}{\partial x_{2}^{2}} &
\cdots &
\dfrac{\partial^{2}f}{\partial x_{2}\partial x_{n}}\\[6pt]
\vdots & \vdots & \ddots & \vdots\\[6pt]
\dfrac{\partial^{2}f}{\partial x_{n}\partial x_{1}} &
\dfrac{\partial^{2}f}{\partial x_{n}\partial x_{2}} &
\cdots &
\dfrac{\partial^{2}f}{\partial x_{n}^{2}}
\end{bmatrix}$$


#### 关键性质

- **对称性**（当二阶偏导数连续时）：  
  $\frac{\partial^{2}f}{\partial x_{i}\partial x_{j}} = \frac{\partial^{2}f}{\partial x_{j}\partial x_{i}}$
  
#### 作用

- **极值判别**：  
  在驻点处  
  - Hessian **正定** → 局部 **极小**  
  - Hessian **负定** → 局部 **极大**  
  - Hessian **不定** → **鞍点**

- **优化算法**：  
  牛顿法、拟牛顿法等利用 Hessian 或其近似做二次近似。

- **二次型**：  
  二次型的系数矩阵就是 Hessian。

#### 二维示例

函数 \(f(x,y)\) 的 Hessian：
$$\mathbf{H}(f)=
\begin{bmatrix}
f_{xx} & f_{xy}\\[4pt]
f_{yx} & f_{yy}
\end{bmatrix}$$
那么判断一个函数是否有极小值的条件即为二阶导数矩阵是否为正定矩阵。
以我们举出的例子，$f(x,y)=2x_{1}^2+12x_{1}x_{2}+20x_{2}7^2$,那么对应的二次导数矩阵是：$\begin{bmatrix}4&12\\12&40\end{bmatrix}$。判断他的是否为正定矩阵即可！

上面我们都是在讨论二维的，同样的当我们把维度上升到n维，以上的所有都会成立！

然后我们举一个3 维椭球体与矩阵联系得到的特殊性质的例子：
$A=\begin{bmatrix}2&-1&0\\-1&2&-1\\0&-1&2\end{bmatrix}$
这个矩阵的二次型是：$2x_1^2+2x_{2}^2+2x_{3}^2-2x_{1}x_{2}-2x_{2}x_{3}$
这个反映到三维空间中就是一个椭球体！此椭球体的三个轴的方向即为 A 矩阵的特征向量方向，三个轴的长度即为特征值的大小。我们可以通过分解 A 矩阵：$A = Q\Lambda𝑄^𝑇$得到其对应值（再次提到这个分解相信大家不再陌生了吧）。这个特殊的性质我们称为：主轴定理。

# 讨论课
给出一个矩阵：$$\begin{bmatrix}2&-1&-1\\-1&2&-1\\-1&-1&2+c\end{bmatrix}$$
请问c分别等于什么值时，矩阵是正定的以及半正定的？
我们在上面说到了4个验证的方法。下面我们一一尝试！
（1）子行列式验证法！（这也是最常用的，如果在考试中，你应该使用这个方法！）
首先这个矩阵拆分为几个子矩阵：$|2|,\begin{bmatrix}2&-1\\-1&2\end{bmatrix},\begin{bmatrix}2&-1&-1\\-1&2&-1\\-1&-1&2+c\end{bmatrix}$
然后分别找出他们的行列式大小，如果都大于0，那么就是正定矩阵，如果是=0，那就是半正定，小于0就是非正定！
第一个矩阵行列式显然是2！
第二个矩阵行列式也很好计算，就是3！
第三个矩阵行列式，我们用代数余子式计算得到：
$2\begin{bmatrix}2&-1\\-1&2+c\end{bmatrix}-(-1)\begin{bmatrix}-1&-1\\-1&2+c\end{bmatrix}+(-1)\begin{bmatrix}-1&2\\-1&-1\end{bmatrix}=3c$
所以c=0为半正定矩阵，>0是正定矩阵！

（2）主元验证法！
当所有主元都大于0时，才是正定矩阵。当没有负主元且至少有一个0主元的情况下是半正定矩阵（主元全是0，那这是一个零矩阵，零矩阵也是半正定矩阵！）。当出现一个负主元时就是非正定矩阵！
那么这个矩阵我们消元得到：$$\begin{bmatrix}2&-1&-1\\-1&2&-1\\-1&-1&2+c\end{bmatrix}→\begin{bmatrix}2&-1&-1\\0&\frac{3}{2}&-\frac{3}{2}\\0&0&c\end{bmatrix}$$
然后我们得到的结果与第一种解法一样！

趁着这个题目，需要强调一个事实：第一个主元就是第一个子行列式的值，第二个主元是第二个子行列式的值除以第一个子行列式的值，第三个主元是第三个子行列式的值除以第二个子行列式的值。这个规律适用于所有情况！

（3）二次型函数大于或者等于0
二次型函数大于0，或者在所有元素都等于0时得到结果为0时，才是正定矩阵。如果是有其他的情况下等于0，那就是半正定矩阵！如果有负数出现就是非正定矩阵！

首先根据矩阵$\begin{bmatrix}2&-1&-1\\-1&2&-1\\-1&-1&2+c\end{bmatrix}$,那么根据二次型的规律写出二次型函数为：$2x^2+2y^2+2z^2-2xy-2xz-2yz$，我写出这个完全没有计算，首先几个平方的系数全是主对角线元素，而对于的其他系数就是对称元素的和！
然后我们对这个二次型函数进行配方得到：$2(x-\frac{1}{2}y-\frac{1}{2}z)^2+\frac{3}{2}(y-z)^2+cz^2$。这个配方也是有技巧的，上面我们讨论过，所有括号外面的系数是主元，然后里面的系数根据$(a+b+c)^2和(a+b)^2$来拼凑！而我们要判断$2(x-\frac{1}{2}y-\frac{1}{2}z)^2+\frac{3}{2}(y-z)^2+cz^2=0$。（他是不能）显然当c>0时，只有x=y=z=0时才等于0，当c=0时，只需要x=y=z即可！怎么显然出来的呢，如果代数直觉好，是可以一眼看出的，如果不好大家抽象出矩阵来看！
还有一个特征值验证法，由于前面我们求结果无数次矩阵特征值，我们就略过！

# 习题课
# 问题一
若 $A$ 与 $B$ 均为**对称正定矩阵**，则 $AB$ 甚至可能**不对称**，但其**所有特征值仍为正**。  
从方程
$$
ABx = \lambda x
$$
出发，**用 $Bx$ 与两边做点积**，（这算是小提示哦！）然后证明 $\lambda > 0$。

解答：
由特征值方程

$$
ABx = \lambda x,
$$

将两边与向量 $Bx$ 做点积，得

$$
(ABx)^T (Bx) = (\lambda x)^T (Bx).
$$

利用转置性质 $(MN)^T = N^T M^T$ 以及 $A$ 和 $B$ 的对称性（即 $A^T = A$，$B^T = B$），左边可化简为

$$
(ABx)^T (Bx) = x^T B^T A^T B x = x^T B A B x = (Bx)^T A (Bx).
$$

右边化简为

$$
(\lambda x)^T (Bx) = \lambda x^T B x.
$$

于是得到

$$
(Bx)^T A (Bx) = \lambda x^T B x.
$$

- 由于 $A$ 正定且 $Bx \neq 0$（因为 $B$ 正定且 $x \neq 0$），有  
  $(Bx)^T A (Bx) > 0$；  
- 又因 $B$ 正定，故 $x^T B x > 0$。

因此

$$
\lambda = \frac{(Bx)^T A (Bx)}{x^T B x} > 0,
$$

即证得所有特征值 $\lambda$ 均为正。

问题二于讨论课内容重合，不再讨论！

